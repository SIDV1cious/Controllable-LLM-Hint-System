import streamlit as st
import os
import random
from sqlalchemy import create_engine, text
from openai import OpenAI
from dotenv import load_dotenv
from datetime import datetime
import pytz
from prompts import SYSTEM_INSTRUCTION, JUDGE_PROMPT_SYSTEM

load_dotenv()


# --- é…ç½®è¯»å– ---
def get_config(key):
    if key in st.secrets:
        return st.secrets[key]
    return os.getenv(key)


api_key = get_config("LLM_API_KEY")
db_user = get_config("DB_USER")
db_password = get_config("DB_PASSWORD")
db_host = get_config("DB_HOST")
db_name = get_config("DB_NAME")
my_id = get_config("MY_ID")

client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

# --- é¢„è®¾é¢˜åº“ (æ‚¨å¯ä»¥éšæ—¶åœ¨è¿™é‡Œæ·»åŠ æ›´å¤šé¢˜ç›®) ---
QUESTION_BANK = [
    {
        "id": 1,
        "category": "é«˜ç­‰æ•°å­¦",
        "content": "å·²çŸ¥å‡½æ•° f(x) = x * ln(x)ï¼Œæ±‚ f(x) åœ¨ x = e å¤„çš„å¯¼æ•°å€¼ã€‚"
    },
    {
        "id": 2,
        "category": "çº¿æ€§ä»£æ•°",
        "content": "æ±‚çŸ©é˜µ A = [[1, 2], [2, 1]] çš„ç‰¹å¾å€¼ã€‚"
    },
    {
        "id": 3,
        "category": "å¾®ç§¯åˆ†",
        "content": "è®¡ç®—ä¸å®šç§¯åˆ† âˆ« x * e^x dxã€‚"
    },
    {
        "id": 4,
        "category": "å¯¼æ•°åº”ç”¨",
        "content": "æ±‚å‡½æ•° y = x^3 - 3x + 1 çš„å•è°ƒé€’å¢åŒºé—´ã€‚"
    }
]

# --- Session State åˆå§‹åŒ– ---
if "current_question" not in st.session_state:
    st.session_state.current_question = QUESTION_BANK[0]  # é»˜è®¤ç¬¬ä¸€é¢˜
if "trial_count" not in st.session_state:
    st.session_state.trial_count = 0
if "messages" not in st.session_state:
    st.session_state.messages = []
if "answer_input" not in st.session_state:
    st.session_state.answer_input = ""
if "check_result" not in st.session_state:
    st.session_state.check_result = None  # ç”¨äºå­˜å‚¨åˆ¤é¢˜ç»“æœçŠ¶æ€


# --- æ•°æ®åº“è¿æ¥ ---
@st.cache_resource
def get_db_engine():
    db_url = f"mysql+pymysql://{db_user}:{db_password}@{db_host}/{db_name}"
    return create_engine(db_url, pool_recycle=1800, pool_pre_ping=True)


def save_to_logs(user_query, ai_response, is_leaking=0):
    # è®°å½•å½“å‰é¢˜ç›®IDï¼ˆå¦‚æœæ˜¯åœ¨é¢˜åº“é‡Œçš„ï¼Œå°±å­˜é¢˜åº“IDï¼‰
    q_id = st.session_state.current_question["id"]
    engine = get_db_engine()
    try:
        with engine.connect() as conn:
            sql = text("""
                       INSERT INTO interaction_logs
                       (question_id, student_id, user_query, ai_response, is_leaking_answer, created_at)
                       VALUES (:q_id, :s_id, :query, :resp, :leaking, :time)
                       """)
            conn.execute(sql, {
                "q_id": q_id,
                "s_id": my_id,
                "query": user_query,
                "resp": ai_response,
                "leaking": is_leaking,
                "time": datetime.now(pytz.timezone('Asia/Shanghai'))
            })
            conn.commit()
    except Exception as e:
        print(f"å­˜è¯å¤±è´¥ï¼š{e}")  # ç”Ÿäº§ç¯å¢ƒä¸å¼¹çª—æ‰“æ‰°ç”¨æˆ·


def generate_report():
    ai_reply_count = len([m for m in st.session_state.messages if m["role"] == "assistant"])
    report = f"# æ¯•è®¾å®éªŒæ•°æ®æŠ¥å‘Š\n- **é¡¹ç›®æ ‡é¢˜**ï¼šåŸºäºDeepseekçš„å¯æ§è§£é¢˜æç¤ºç”Ÿæˆç³»ç»Ÿ\n"
    report += f"- **è´Ÿè´£äºº**ï¼šå·¦æ¢“æ¡ ({my_id})\n"
    report += f"- **å¯¼å‡ºæ—¶é—´**ï¼š{datetime.now(pytz.timezone('Asia/Shanghai')).strftime('%Y-%m-%d %H:%M')}\n"
    report += f"## å…³é”®æ•°æ®æŒ‡æ ‡\n- **ç­”æ¡ˆæäº¤æ¬¡æ•°**ï¼š{st.session_state.trial_count} æ¬¡\n- **æ™ºèƒ½è¾…å¯¼æ¬¡æ•°**ï¼š{ai_reply_count} æ¬¡\n"
    return report


# --- é¡µé¢è®¾ç½® (ç§»é™¤èƒŒæ™¯è‰²CSSï¼Œå›å½’é»˜è®¤äº®è‰²) ---
st.set_page_config(page_title="æ™ºèƒ½å¯¼å­¦ç³»ç»Ÿ", layout="wide", initial_sidebar_state="expanded")

# --- ä¾§è¾¹æ ï¼šé€‰é¢˜åŒº ---
with st.sidebar:
    st.header("ğŸ“š é¢˜åº“é€‰æ‹©")
    st.info(f"å½“å‰ç”¨æˆ·ï¼š{my_id}")

    # æ–¹å¼1ï¼šä¸‹æ‹‰é€‰æ‹©
    selected_q_title = st.selectbox(
        "é€‰æ‹©é¢˜ç›®ï¼š",
        options=[f"[{q['category']}] é¢˜ç›® {q['id']}" for q in QUESTION_BANK],
        index=QUESTION_BANK.index(st.session_state.current_question)
    )

    # è§£æé€‰æ‹©çš„é¢˜ç›®ID
    selected_id = int(selected_q_title.split("é¢˜ç›® ")[1])

    # æ£€æŸ¥æ˜¯å¦åˆ‡æ¢äº†é¢˜ç›®
    if selected_id != st.session_state.current_question["id"]:
        st.session_state.current_question = next(q for q in QUESTION_BANK if q["id"] == selected_id)
        # åˆ‡æ¢é¢˜ç›®æ—¶é‡ç½®æ‰€æœ‰çŠ¶æ€
        st.session_state.messages = []
        st.session_state.trial_count = 0
        st.session_state.answer_input = ""
        st.session_state.check_result = None
        st.rerun()

    st.divider()

    # æ–¹å¼2ï¼šéšæœºæŠ½é¢˜
    if st.button("ğŸ² éšæœºæŠ½å–ä¸€é¢˜"):
        new_q = random.choice(QUESTION_BANK)
        # é¿å…éšæœºåˆ°åŒä¸€é¢˜ï¼ˆå¦‚æœæ˜¯åŒä¸€é¢˜å°±å†éšä¸€æ¬¡ï¼Œç®€å•å¤„ç†ï¼‰
        if new_q["id"] == st.session_state.current_question["id"]:
            new_q = random.choice(QUESTION_BANK)

        st.session_state.current_question = new_q
        st.session_state.messages = []
        st.session_state.trial_count = 0
        st.session_state.answer_input = ""
        st.session_state.check_result = None
        st.rerun()

    st.divider()
    st.download_button(label="ğŸ“¥ å¯¼å‡ºå­¦ä¹ æŠ¥å‘Š", data=generate_report(), file_name=f"report_{my_id}.md")

# --- ä¸»ç•Œé¢ ---
st.title("ğŸ“ æ™ºèƒ½å¯¼å­¦ä¸åˆ¤é¢˜ç³»ç»Ÿ")

# é¡¶éƒ¨æŒ‡æ ‡æ 
col_m1, col_m2, col_m3 = st.columns(3)
col_m1.metric("å½“å‰ç§‘ç›®", st.session_state.current_question["category"])
col_m2.metric("å°è¯•æ¬¡æ•°", st.session_state.trial_count)
ai_count = len([m for m in st.session_state.messages if m["role"] == "assistant"])
col_m3.metric("è·å¾—è¾…å¯¼", f"{ai_count} æ¬¡")

st.divider()

# é¢˜ç›®æ˜¾ç¤ºåŒº
st.subheader("ğŸ“ å½“å‰é¢˜ç›®")
st.info(st.session_state.current_question["content"], icon="ğŸ§")

# ç­”æ¡ˆè¾“å…¥åŒº
st.subheader("âœï¸ ä½ çš„è§£ç­”")
student_answer = st.text_area("åœ¨æ­¤è¾“å…¥ä½ çš„è§£é¢˜è¿‡ç¨‹æˆ–æœ€ç»ˆç­”æ¡ˆï¼š", height=150, key="answer_input")

# æäº¤æŒ‰é’®åŒº
col_submit, col_hint = st.columns([1, 4])
with col_submit:
    if st.button("ğŸš€ æäº¤åˆ¤é¢˜", type="primary", use_container_width=True):
        if not student_answer.strip():
            st.warning("è¯·å…ˆè¾“å…¥ç­”æ¡ˆå†æäº¤ï¼")
        else:
            # åˆ¤é¢˜é€»è¾‘
            st.session_state.trial_count += 1
            judge_prompt = f"é¢˜ç›®ï¼š{st.session_state.current_question['content']}\nå­¦ç”Ÿç­”æ¡ˆï¼š{student_answer}\nåˆ¤æ–­å¯¹é”™ã€‚åªèƒ½è¾“å‡º'æ­£ç¡®'æˆ–'é”™è¯¯'ã€‚"

            try:
                response = client.chat.completions.create(model="deepseek-chat", messages=[
                    {"role": "system", "content": JUDGE_PROMPT_SYSTEM},
                    {"role": "user", "content": judge_prompt}])
                result = response.choices[0].message.content.strip()

                is_correct = "æ­£ç¡®" in result
                if is_correct:
                    st.session_state.check_result = "correct"
                    st.toast("æ­å–œä½ ï¼Œç­”æ¡ˆæ­£ç¡®ï¼", icon="âœ…")
                    save_to_logs(f"ã€ç­”æ¡ˆæäº¤ã€‘{student_answer}", "æ­£ç¡®")
                else:
                    st.session_state.check_result = "wrong"
                    st.toast("ç­”æ¡ˆæœ‰è¯¯ï¼Œè¯·å‚è€ƒä¸‹æ–¹æ™ºèƒ½è¾…å¯¼ã€‚", icon="âŒ")
                    save_to_logs(f"ã€ç­”æ¡ˆæäº¤ã€‘{student_answer}", "é”™è¯¯")

                st.rerun()  # åˆ·æ–°ä»¥æ›´æ–°é¡¶éƒ¨æŒ‡æ ‡å’Œä¸‹æ–¹çŠ¶æ€

            except Exception as e:
                st.error(f"åˆ¤é¢˜æœåŠ¡è¿æ¥å¤±è´¥ï¼š{e}")

# æ˜¾ç¤ºåˆ¤é¢˜ç»“æœåé¦ˆï¼ˆæŒä¹…åŒ–æ˜¾ç¤ºï¼‰
if st.session_state.check_result == "correct":
    st.success("âœ… å›ç­”æ­£ç¡®ï¼ä½ å·²æŒæ¡è¯¥çŸ¥è¯†ç‚¹ã€‚")
elif st.session_state.check_result == "wrong":
    st.error("âŒ å›ç­”é”™è¯¯ã€‚åˆ«ç°å¿ƒï¼Œåœ¨ä¸‹æ–¹ä¸ AI åŠ©æ•™è®¨è®ºä¸€ä¸‹å§ğŸ‘‡")

# --- æ™ºèƒ½è¾…å¯¼åŒº (Chat) ---
st.divider()
st.subheader("ğŸ¤– æ™ºèƒ½åŠ©æ•™ (AI Tutor)")

# æ˜¾ç¤ºå†å²è®°å½•
for message in st.session_state.messages:
    avatar = "ğŸ§‘â€ğŸ“" if message["role"] == "user" else "ğŸ¤–"
    with st.chat_message(message["role"], avatar=avatar):
        st.markdown(message["content"])

# èŠå¤©è¾“å…¥æ¡†
if prompt := st.chat_input("å¯¹è¿™é“é¢˜æœ‰ç–‘é—®ï¼Ÿè¾“å…¥ 'æ€ä¹ˆåš' æˆ– 'ç»™æˆ‘ç‚¹æç¤º'"):
    # å­˜å…¥ç”¨æˆ·æ¶ˆæ¯
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user", avatar="ğŸ§‘â€ğŸ“"):
        st.markdown(prompt)

    # æ„é€  Prompt
    context = f"ã€é¢˜ç›®ã€‘ï¼š{st.session_state.current_question['content']}\nã€å­¦ç”Ÿå½“å‰é”™é¢˜æœ¬ã€‘ï¼š{student_answer}\nã€å­¦ç”Ÿç–‘é—®ã€‘ï¼š{prompt}"

    with st.chat_message("assistant", avatar="ğŸ¤–"):
        response_placeholder = st.empty()
        full_response = ""

        try:
            stream = client.chat.completions.create(
                model="deepseek-chat",
                messages=[
                    {"role": "system", "content": SYSTEM_INSTRUCTION},
                    {"role": "user", "content": context}
                ],
                stream=True
            )

            for chunk in stream:
                content = chunk.choices[0].delta.content
                if content:
                    full_response += content
                    display_text = full_response.replace(r"\[", "$$").replace(r"\]", "$$").replace(r"\(", "$").replace(
                        r"\)", "$")
                    response_placeholder.markdown(display_text + "â–Œ")

            final_text = full_response.replace(r"\[", "$$").replace(r"\]", "$$").replace(r"\(", "$").replace(r"\)", "$")
            response_placeholder.markdown(final_text)

            st.session_state.messages.append({"role": "assistant", "content": final_text})
            save_to_logs(f"ã€æ™ºèƒ½è¾…å¯¼ã€‘{prompt}", final_text)

            # åˆ·æ–°é¡µé¢ä»¥æ›´æ–°é¡¶éƒ¨çš„â€œæ™ºèƒ½è¾…å¯¼æ¬¡æ•°â€
            st.rerun()

        except Exception as e:
            st.error(f"AI å“åº”ä¸­æ–­ï¼š{e}")

# åº•éƒ¨ç‰ˆæƒ
st.markdown("---")
st.caption(f"Â© 2026 æ™ºèƒ½å¯¼å­¦ç³»ç»Ÿ | å­¦ç”Ÿï¼šå·¦æ¢“æ¡ | æŒ‡å¯¼æ•™å¸ˆï¼šç‹å»ºè£")